# GameTheoryNeuralNetwork

Classifiers are used in security to classify the actions of an adversary as malicious or benign. This interaction can be modeled as a game; the strategy of one player corresponds to setting parameters of a classifier, the strategy of the opponent is to choose such an input that causes misclassification. Recently, there has been a large volume of work on verification of classifiers and generating adversarial samples. However, it is not clear whether these approaches are compatible with reward functions and strategy constraints the attacker typically has. The goal of this double-oracle framework is to provide experimental analysis and identify advantages and disadvantages of existing verification methods and methods for generating adversarial samples, which are compatible with reward functions and strategy constraints.
